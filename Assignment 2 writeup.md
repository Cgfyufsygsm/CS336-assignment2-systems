# Assignment 2 (systems) Writeup

## Profiling and Benchmarking

### `benchmarking_script`

#### (a) 脚本实现

我实现了一个可参数化的 benchmark 脚本，支持按给定超参数初始化模型、生成随机输入 batch、执行 warm-up 步骤后再进行正式计时，并在每步结束后调用 `torch.cuda.synchronize()` 以避免 CUDA 异步导致的计时偏差。脚本支持 `forward`、`forward-backward` 和 `train-step` 三种模式，其中本题使用 `forward-backward` 来分别统计前向与反向耗时，并将结果输出为 JSON 便于后续汇总。

#### (b) benchmarking

> 实验环境：`NVIDIA H800 PCIe`，`batch_size=4`，`warm-up=5`，`measure=10`
>
> 单位：ms（均值 ± 标准差）

| Model Size | Context |       Forward |      Backward |
| ---------- | ------: | ------------: | ------------: |
| small      |     128 |  21.57 ± 0.13 |  23.02 ± 1.06 |
| medium     |     128 |  45.03 ± 1.10 |  46.51 ± 0.47 |
| large      |     128 |  64.98 ± 0.69 |  93.93 ± 0.05 |
| xl         |     128 |  87.18 ± 1.23 | 175.88 ± 0.18 |
| 2.7b       |     128 | 119.78 ± 0.71 | 264.10 ± 0.75 |

对于 context 变化的补充（同样 `warm-up=5`, `measure=10`）：

| Model Size | Context=256 (F/B) | Context=512 (F/B) |
| ---------- | ----------------: | ----------------: |
| small      |     21.86 / 26.00 |     27.24 / 54.23 |
| medium     |     44.37 / 81.12 |    84.28 / 164.97 |
| large      |    86.51 / 170.95 |   161.45 / 342.85 |
| xl         |   152.54 / 321.49 |   326.48 / 665.57 |
| 2.7b       |   232.57 / 487.16 |  490.07 / 1003.19 |

结论：随着模型规模和 context length 增大，forward/backward 耗时都显著上升；在小模型上 backward 仅略高于 forward，但在大模型（尤其 `xl`、`2.7b`）上 backward 明显更重。整体波动较小，大多数配置的标准差维持在低毫秒量级。

#### (c) warm-up

> 单位：ms（均值 ± 标准差）
> 实验环境：5090

| Model Size | Warm-up |         Forward |       Backward |
| ---------- | ------: | --------------: | -------------: |
| small      |       0 |  71.46 ± 138.07 |  30.60 ± 37.73 |
| small      |       1 |   33.54 ± 43.82 |   22.82 ± 3.18 |
| small      |       2 |   32.45 ± 39.19 |   25.24 ± 6.83 |
| medium     |       0 | 104.75 ± 206.12 |  57.04 ± 48.73 |
| medium     |       1 |    47.29 ± 9.32 |   47.87 ± 6.53 |
| medium     |       2 |    32.89 ± 1.52 |   34.60 ± 1.13 |
| large      |       0 | 113.16 ± 189.20 |  77.75 ± 48.34 |
| large      |       1 |    52.67 ± 5.29 |   64.52 ± 3.04 |
| large      |       2 |    53.73 ± 1.56 |   65.99 ± 0.82 |
| xl         |       0 | 122.53 ± 182.62 | 128.62 ± 35.63 |
| xl         |       1 |    63.73 ± 8.23 |  118.83 ± 0.23 |
| xl         |       2 |    65.76 ± 4.05 |  121.82 ± 0.08 |
| 2.7b       |       0 | 137.96 ± 181.38 | 179.41 ± 49.02 |
| 2.7b       |       1 |    81.74 ± 0.73 |  165.86 ± 0.90 |
| 2.7b       |       2 |    81.58 ± 0.09 |  165.02 ± 0.69 |

结论：不做 warm-up（warm-up=0）时，forward/backward 的平均耗时明显偏高，而且标准差显著变大，结果不稳定。主要原因是初始迭代会包含额外开销（如 CUDA 上下文初始化、kernel 选择与缓存、内存分配器预热等），导致首批 step 不能代表稳态性能。即使做 1-2 步 warm-up，结果仍可能不同，因为有些配置需要更多步才能完全进入稳态，且系统噪声也会带来残余波动。

### `nsys_profile`

#### (a) forward pass

| Model Size | Context | Forward (Python) | Forward (nsight) |
| ---------- | ------: | ---------------: | ---------------: |
| small      |     128 |     15.57 ± 0.41 |           18.382 |
| medium     |     128 |     29.02 ± 0.09 |           38.213 |
| large      |     128 |     43.23 ± 0.21 |           54.855 |
| xl         |     128 |     59.66 ± 0.03 |           74.066 |
| 2.7b       |     128 |     81.04 ± 0.07 |           53.136 |

还是比较不一样的，推测是因为 kernel 的问题。

#### (b) kernel

| Model Size | Context |                                                       Kernel |  Time | Invoked |
| ---------- | ------: | -----------------------------------------------------------: | ----: | ------- |
| small      |     128 | `void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_8x4_tn_align1>(T1::Params)` | 55.0% | 60      |
| medium     |     128 | ` void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_8x4_tn_align1>(T1::Params)` | 45.2% | 120     |
| large      |     128 | `void cutlass::Kernel2<cutlass_80_simt_sgemm_128x256_8x4_tn_align1>(T1::Params)` | 52.6% | 107     |
| xl         |     128 | `void cutlass::Kernel2<cutlass_80_simt_sgemm_128x256_8x4_tn_align1>(T1::Params)` | 56.5% | 143     |
| 2.7b       |     128 | `void cutlass::Kernel2<cutlass_80_simt_sgemm_128x256_8x4_tn_align1>(T1::Params)` | 90.2% | 148     |

确实还是同一个 kernel 在 backward 中耗最长时间，但是比例明显降低了。

#### (c) other kernel

比如 element-wise 乘法的 kernel。

#### (d) training

矩阵乘法的比例明显下降了，从之前的将近一半下降到只有 10-20%。处理 element-wise 的乘法的 kernel 比例上升。

#### (e) softmax vs. mm

可以发现 softmax 还是需要相当时间的，略低于计算 attention score 的时间，略低于 final matmul 的 二倍。

### `mixed_precision_accumulation`

运行结果如下：

```
tensor(10.0001)
tensor(9.9531, dtype=torch.float16)
tensor(10.0021)
tensor(10.0021)
```

可以发现，如果全程使用 `torch.float32`，则舍入误差最小；若全程使用 `torch.float16`，有效位数少导致误差累积增大；后面两种为混合精度，只要 accumulation 在 `float32` 中进行则误差仍然在可接受范围内。

### `benchmarking_mixed_precision`

#### (a) data type

- 模型参数仍是 FP32
- `ToyModel.fc1` 的输出：FP16
- `ToyModel.ln`  输出：FP32
- logits：FP32
- loss：FP32
- 梯度：FP32

#### (b) LayerNorm presicion analysis

在 FP16 mixed precision 下，LayerNorm 最敏感的是均值/方差的归约与归一化计算（减均值、平方、求和、rsqrt/除法），因为这些操作会累积舍入误差，且容易受 FP16 有效精度与动态范围限制影响。LayerNorm 的统计量若用低精度算，数值误差会被放大并影响训练稳定性，所以通常会在 FP32 中完成这些计算。换成 BF16 后由于其指数位与 FP32 相同，溢出/下溢问题明显缓解，但 BF16 尾数仍较短，精度仍不如 FP32，因此实践中通常仍让 LayerNorm 的统计与归约保持 FP32 更稳妥。

#### (c) mixed precision benchmarking

> 实验环境：`NVIDIA H800 PCIe`，`context_length=128`，`batch_size=4`，`warm-up=5`，`measure=10`
>
> 单位：ms（均值）；Speedup = FP32 / BF16

| Model Size | Fwd FP32 | Fwd BF16 | Fwd Speedup | Bwd FP32 | Bwd BF16 | Bwd Speedup |
| ---------- | -------: | -------: | ----------: | -------: | -------: | ----------: |
| small      |    21.57 |    23.54 |       0.92x |    23.02 |    24.97 |       0.92x |
| medium     |    45.03 |    46.80 |       0.96x |    46.51 |    49.65 |       0.94x |
| large      |    64.98 |    68.18 |       0.95x |    93.93 |    73.93 |       1.27x |
| xl         |    87.18 |    92.29 |       0.94x |   175.88 |    98.59 |       1.78x |
| 2.7b       |   119.78 |    62.11 |       1.93x |   264.10 |    86.81 |       3.04x |

在当前 GPU 上，BF16 对小模型（`small`/`medium`）的 forward 和 backward 都没有提速，甚至略慢；但随着模型增大，BF16 的收益迅速增强，尤其是 backward。以 `2.7b` 为例，forward 从 119.78ms 降到 62.11ms（1.93x），backward 从 264.10ms 降到 86.81ms（3.04x）。整体趋势是模型越大，mixed precision（BF16）越能发挥 Tensor Core 的优势，且在 backward 阶段更明显。

### `memory_profiling`

> 实验设置：`NVIDIA H800 PCIe`，`batch_size=4`，`warm-up=5`，`measure=10`，`context_length in {128, 256, 512}`
>
> 峰值内存来自 `outputs/memory_profiles/{fp32,bf16}` 的 memory snapshot（`torch.cuda.memory._dump_snapshot`），单位统一为 GiB（1 GiB = 1024^3 bytes）。

#### (a) active memory timeline analysis

![](https://yangty-pic.oss-cn-beijing.aliyuncs.com/cs336/lab2/memory-profiling-ctx512-forward.png)

![](https://yangty-pic.oss-cn-beijing.aliyuncs.com/cs336/lab2/memory-profiling-ctx512-train.png)

从时间线看，forward 图呈现明显“先上升后下降”的单峰形状，符合前向中激活逐层累积再释放的行为；train-step 图在更高内存区间内先升到峰值、再分阶段回落并最终稳定在较高平台，能和 forward/backward/optimizer 的阶段性行为对应起来。尤其是 train-step 中后段的锯齿与平台，反映了梯度与优化器相关张量的反复分配和保留。

#### (b) peak memory with FP32

FP32 下 2.7B 的峰值内存如下：

| Context Length | Forward Peak (GiB) | Train-step Peak (GiB) |
| -------------- | -----------------: | --------------------: |
| 128            |              18.08 |                 51.44 |
| 256            |              24.31 |                 51.44 |
| 512            |              39.77 |                 65.52 |

结论：forward 峰值随 context 明显上升；train-step 在 128/256 时峰值接近，但到 512 时显著抬升。

#### (c) peak memory with BF16 mixed

BF16 mixed precision 下峰值（同一模型与 context）：

| Context Length | Forward Peak (GiB) | Train-step Peak (GiB) |
| -------------- | -----------------: | --------------------: |
| 128            |              22.45 |                 51.44 |
| 256            |              26.50 |                 52.09 |
| 512            |              36.86 |                 62.69 |

与 FP32 对比：mixed precision 对峰值内存并非在所有配置都显著下降。小 context（128/256）下 forward 甚至更高或接近，train-step 也基本接近；到 context=512 时，BF16 才出现较明确收益（forward 39.77 -> 36.86 GiB，train-step 65.52 -> 62.69 GiB）。这说明峰值内存并不只由激活 dtype 决定，还受优化器状态、缓存与分配行为影响。

#### (d) activation tensor size

Transformer residual stream 的激活张量形状是 `(B, T, d_model)`，单精度字节数：

```
size_bytes = B * T * d_model * 4
```

在本作业参考配置中 `B=4, d_model=2560`，所以：

```
size_MB = 4 * T * 2560 * 4 / 1024^2 = 0.0390625 * T
```

因此 `T=128/256/512` 时分别为 `5/10/20 MB`（MiB）。

#### (e) largest allocation

在 forward 的 memory timeline（例如 `ctx512_forward.pickle`）把 Detail 降到 10% 后，最大的分配块大小是 **128 MiB**。从 stack trace 看，这些大块主要来自 self-attention 的 `scaled_dot_product_attention` 路径，对应的是注意力分数/概率这类 `B x H x T x T` 级别张量的分配。

## Optimizing Attention with FlashAttention-2

### `pytorch_attention`

> 实验环境：`1 * NVIDIA H800 PCIe`。

| d_model | seq_len | forward ms (mean±std) | backward ms (mean±std) | mem before bwd (MiB) | peak mem (MiB) |
| ------: | ------: | --------------------: | ---------------------: | -------------------: | -------------: |
|      16 |     256 |         0.147 ± 0.003 |          0.417 ± 0.006 |                68.59 |          76.71 |
|      16 |    1024 |         0.334 ± 0.003 |          0.886 ± 0.009 |               131.09 |         259.59 |
|      16 |    4096 |         4.567 ± 0.010 |         14.395 ± 0.425 |              1112.38 |        3162.38 |
|      16 |    8192 |        17.767 ± 0.024 |         59.494 ± 1.732 |              4240.75 |       12436.75 |
|      16 |   16384 |        70.177 ± 0.062 |        233.282 ± 6.917 |             16737.50 |       49513.50 |
|      32 |     256 |         0.144 ± 0.004 |          0.545 ± 0.012 |                69.09 |          77.34 |
|      32 |    1024 |         0.346 ± 0.002 |          0.913 ± 0.024 |               133.09 |         262.09 |
|      32 |    4096 |         4.701 ± 0.013 |         14.639 ± 0.430 |              1120.38 |        3172.38 |
|      32 |    8192 |        18.430 ± 0.064 |         57.968 ± 1.806 |              4256.75 |       12456.75 |
|      32 |   16384 |        73.361 ± 0.041 |        230.936 ± 7.252 |             16769.50 |       49553.50 |
|      64 |     256 |         0.144 ± 0.004 |          0.551 ± 0.013 |                70.09 |          78.59 |
|      64 |    1024 |         0.379 ± 0.007 |          0.986 ± 0.010 |               137.09 |         267.09 |
|      64 |    4096 |         5.120 ± 0.017 |         15.918 ± 0.470 |              1136.38 |        3192.38 |
|      64 |    8192 |        19.990 ± 0.112 |         62.409 ± 1.945 |              4288.75 |       12496.75 |
|      64 |   16384 |        79.836 ± 0.119 |        250.914 ± 7.913 |             16833.50 |       49633.50 |
|     128 |     256 |         0.144 ± 0.003 |          0.420 ± 0.006 |                72.09 |          81.09 |
|     128 |    1024 |         0.469 ± 0.018 |          1.246 ± 0.019 |               145.09 |         277.09 |
|     128 |    4096 |         5.986 ± 0.013 |         18.652 ± 0.569 |              1168.38 |        3232.38 |
|     128 |    8192 |        24.207 ± 0.247 |         74.089 ± 2.344 |              4352.75 |       12576.75 |
|     128 |   16384 |        94.780 ± 0.472 |        291.347 ± 9.281 |             16961.50 |       49793.50 |

这个 benchmark 显示了朴素注意力的典型特征：时间与显存都被 $L\times L$ 的注意力矩阵主导。对所有 `d_model`，前向和反向耗时随 `seq_len` 增长非常快，而随 `d_model` 增长相对慢很多。例如在 `d_model=128` 下，前向耗时从 `L=4096` 的 `5.99 ms` 增至 `L=8192` 的 `24.21 ms`，再到 `L=16384` 的 `94.78 ms`；反向也表现出接近“ $L$ 翻倍、耗时约 4 倍”的趋势。

当前 GPU 下未出现 OOM。但观察显存消耗也可以发现其增长趋势也是 $O(L^2)$，要消除这部分内存成本，核心做法是避免显式物化并保存完整 $L\times L$ 注意力矩阵，例如采用 FlashAttention 风格的分块计算（或重计算/checkpoint 思路），以更多计算换更低激活显存。

### `torch_compile`

#### (a) attention

| d_model | seq_len | forward eager(ms) | forward compile(ms) | forward speedup | backward eager(ms) | backward compile(ms) | backward speedup |
| ------: | ------: | ----------------: | ------------------: | --------------: | -----------------: | -------------------: | ---------------: |
|      16 |     256 |             0.147 |               0.127 |          1.154x |              0.417 |                0.351 |           1.188x |
|      16 |    1024 |             0.334 |               0.195 |          1.712x |              0.886 |                0.484 |           1.832x |
|      16 |    4096 |             4.567 |               2.240 |          2.038x |             14.395 |                6.302 |           2.284x |
|      16 |    8192 |            17.767 |               8.721 |          2.037x |             59.494 |               27.866 |           2.135x |
|      16 |   16384 |            70.177 |              25.981 |          2.701x |            233.282 |               98.637 |           2.365x |
|      32 |     256 |             0.144 |               0.280 |          0.516x |              0.545 |                0.735 |           0.741x |
|      32 |    1024 |             0.346 |               0.412 |          0.839x |              0.913 |                0.817 |           1.117x |
|      32 |    4096 |             4.701 |               1.961 |          2.398x |             14.639 |                5.887 |           2.487x |
|      32 |    8192 |            18.430 |               7.396 |          2.492x |             57.968 |               23.731 |           2.443x |
|      32 |   16384 |            73.361 |              29.497 |          2.487x |            230.936 |               97.754 |           2.362x |
|      64 |     256 |             0.144 |               0.283 |          0.510x |              0.551 |                0.733 |           0.752x |
|      64 |    1024 |             0.379 |               0.435 |          0.872x |              0.986 |                0.771 |           1.279x |
|      64 |    4096 |             5.120 |               2.430 |          2.107x |             15.918 |                7.224 |           2.204x |
|      64 |    8192 |            19.990 |               9.340 |          2.140x |             62.409 |               28.902 |           2.159x |
|      64 |   16384 |            79.836 |              36.924 |          2.162x |            250.914 |              119.268 |           2.104x |
|     128 |     256 |             0.144 |               0.138 |          1.046x |              0.420 |                0.336 |           1.250x |
|     128 |    1024 |             0.469 |               0.415 |          1.131x |              1.246 |                0.763 |           1.634x |
|     128 |    4096 |             5.986 |               3.365 |          1.779x |             18.652 |               10.599 |           1.760x |
|     128 |    8192 |            24.207 |              13.864 |          1.746x |             74.089 |               42.101 |           1.760x |
|     128 |   16384 |            94.780 |              53.214 |          1.781x |            291.347 |              163.079 |           1.787x |

Attention 子模块在长序列下加速明显，`seq_len>=4096` 时前后向普遍接近 `~1.7x-2.7x`。

#### (b) Transformer

| model size | forward eager (ms) | forward compile (ms) | forward speedup | backward eager (ms) | backward compile (ms) | backward speedup |
| :--------- | -----------------: | -------------------: | --------------: | ------------------: | --------------------: | ---------------: |
| small      |             27.239 |               19.828 |          1.374x |              54.233 |                40.509 |           1.339x |
| medium     |             84.276 |               68.714 |          1.226x |             164.967 |               133.447 |           1.236x |
| large      |            161.453 |              136.453 |          1.183x |             342.850 |               281.020 |           1.220x |
| xl         |            326.482 |              279.752 |          1.167x |             665.574 |               566.427 |           1.175x |
| 2.7b       |            490.072 |              448.186 |          1.093x |            1003.195 |               916.464 |           1.095x |

端到端整模型也有稳定收益，但随模型变大加速比有所下降（`small` 约 `1.3x`，`2.7b` 约 `1.09x`）。

### `flash_benchmarking`

> 实验环境：`1 * H800`（没有 H100）

数据如下，时间单位为 ms。N/A 则是遇到 OOM 的情况。

| dtype    | seq_len | d_model | t_fwd | f_fwd | fwd_x | t_bwd | f_bwd | bwd_x | t_e2e | f_e2e | e2e_x |
| -------- | ------: | ------: | -----------: | -----------: | ----------: | -----------: | -----------: | ----------: | -----------: | -----------: | ----------: |
| bfloat16 |     128 |      16 |     0.03 |     0.01 |    3.80 |     0.12 |     0.27 |    0.44 |     0.32 |     0.41 |    0.78 |
| bfloat16 |     256 |      16 |     0.03 |     0.01 |    3.38 |     0.13 |     0.31 |    0.42 |     0.39 |     0.39 |    1.00 |
| bfloat16 |     512 |      16 |     0.03 |     0.01 |    2.77 |     0.17 |     0.28 |    0.59 |     0.39 |     0.41 |    0.95 |
| bfloat16 |    1024 |      16 |     0.04 |     0.02 |    2.27 |     0.16 |     0.31 |    0.51 |     0.40 |     0.42 |    0.96 |
| bfloat16 |    2048 |      16 |     0.08 |     0.03 |    2.38 |     0.16 |     0.31 |    0.52 |     0.41 |     0.43 |    0.95 |
| bfloat16 |    4096 |      16 |     0.30 |     0.08 |    3.87 |     0.30 |     0.44 |    0.69 |     0.59 |     0.56 |    1.06 |
| bfloat16 |    8192 |      16 |     1.21 |     0.22 |    5.50 |     1.15 |     1.60 |    0.72 |     2.34 |     1.83 |    1.28 |
| bfloat16 |   16384 |      16 |     4.01 |     0.70 |    5.75 |     4.40 |     5.72 |    0.77 |     8.38 |     6.50 |    1.29 |
| bfloat16 |   32768 |      16 |    16.01 |     2.43 |    6.58 |    18.65 |    22.50 |    0.83 |    34.62 |    25.18 |    1.37 |
| bfloat16 |   65536 |      16 |    64.11 |     9.42 |    6.80 |    74.31 |    90.86 |    0.82 |   138.23 |   100.31 |    1.38 |
| bfloat16 |     128 |      32 |     0.03 |     0.01 |    3.65 |     0.16 |     0.27 |    0.60 |     0.39 |     0.40 |    0.99 |
| bfloat16 |     256 |      32 |     0.03 |     0.01 |    2.95 |     0.18 |     0.29 |    0.61 |     0.39 |     0.42 |    0.93 |
| bfloat16 |     512 |      32 |     0.03 |     0.01 |    2.43 |     0.17 |     0.29 |    0.59 |     0.40 |     0.38 |    1.05 |
| bfloat16 |    1024 |      32 |     0.05 |     0.02 |    2.02 |     0.18 |     0.29 |    0.61 |     0.43 |     0.40 |    1.07 |
| bfloat16 |    2048 |      32 |     0.08 |     0.04 |    1.99 |     0.16 |     0.29 |    0.56 |     0.38 |     0.40 |    0.94 |
| bfloat16 |    4096 |      32 |     0.30 |     0.10 |    2.90 |     0.30 |     0.48 |    0.63 |     0.59 |     0.58 |    1.02 |
| bfloat16 |    8192 |      32 |     1.21 |     0.29 |    4.14 |     1.15 |     1.63 |    0.71 |     2.35 |     1.93 |    1.22 |
| bfloat16 |   16384 |      32 |     4.03 |     0.92 |    4.39 |     4.41 |     6.25 |    0.71 |     8.43 |     7.46 |    1.13 |
| bfloat16 |   32768 |      32 |    16.03 |     3.21 |    5.00 |    18.67 |    25.03 |    0.75 |    34.65 |    28.25 |    1.23 |
| bfloat16 |   65536 |      32 |    64.47 |    12.52 |    5.15 |    74.52 |   101.57 |    0.73 |   138.92 |   113.77 |    1.22 |
| bfloat16 |     128 |      64 |     0.03 |     0.01 |    3.17 |     0.16 |     0.27 |    0.59 |     0.38 |     0.40 |    0.94 |
| bfloat16 |     256 |      64 |     0.03 |     0.01 |    2.25 |     0.16 |     0.29 |    0.56 |     0.43 |     0.39 |    1.09 |
| bfloat16 |     512 |      64 |     0.03 |     0.02 |    1.70 |     0.15 |     0.29 |    0.53 |     0.37 |     0.40 |    0.93 |
| bfloat16 |    1024 |      64 |     0.05 |     0.03 |    1.33 |     0.14 |     0.28 |    0.50 |     0.33 |     0.41 |    0.79 |
| bfloat16 |    2048 |      64 |     0.08 |     0.06 |    1.26 |     0.17 |     0.29 |    0.59 |     0.37 |     0.43 |    0.86 |
| bfloat16 |    4096 |      64 |     0.30 |     0.15 |    1.99 |     0.30 |     0.59 |    0.51 |     0.59 |     0.74 |    0.80 |
| bfloat16 |    8192 |      64 |     1.21 |     0.39 |    3.09 |     1.16 |     2.12 |    0.55 |     2.36 |     2.52 |    0.93 |
| bfloat16 |   16384 |      64 |     4.03 |     1.37 |    2.93 |     4.43 |     9.06 |    0.49 |     8.43 |    10.64 |    0.79 |
| bfloat16 |   32768 |      64 |    16.04 |     4.61 |    3.48 |    18.72 |    38.59 |    0.49 |    34.76 |    43.04 |    0.81 |
| bfloat16 |   65536 |      64 |    64.50 |    17.31 |    3.73 |    74.71 |   149.20 |    0.50 |   139.24 |   160.75 |    0.87 |
| bfloat16 |     128 |     128 |     0.03 |     0.01 |    2.77 |     0.13 |     0.25 |    0.52 |     0.33 |     0.34 |    0.96 |
| bfloat16 |     256 |     128 |     0.03 |     0.02 |    1.95 |     0.12 |     0.24 |    0.49 |     0.33 |     0.35 |    0.94 |
| bfloat16 |     512 |     128 |     0.06 |     0.02 |    2.46 |     0.13 |     0.24 |    0.53 |     0.33 |     0.36 |    0.90 |
| bfloat16 |    1024 |     128 |     0.05 |     0.04 |    1.06 |     0.20 |     0.26 |    0.77 |     0.35 |     0.40 |    0.88 |
| bfloat16 |    2048 |     128 |     0.08 |     0.08 |    0.97 |     0.14 |     0.31 |    0.46 |     0.37 |     0.39 |    0.95 |
| bfloat16 |    4096 |     128 |     0.31 |     0.22 |    1.39 |     0.32 |     0.93 |    0.34 |     0.61 |     1.15 |    0.54 |
| bfloat16 |    8192 |     128 |     1.22 |     0.57 |    2.12 |     1.18 |     3.39 |    0.35 |     2.38 |     4.06 |    0.59 |
| bfloat16 |   16384 |     128 |     4.05 |     1.95 |    2.08 |     4.48 |    12.91 |    0.35 |     8.52 |    15.18 |    0.56 |
| bfloat16 |   32768 |     128 |    16.02 |     6.74 |    2.38 |    18.74 |    55.15 |    0.34 |    34.72 |    60.69 |    0.57 |
| bfloat16 |   65536 |     128 |    65.75 |    27.06 |    2.43 |    76.14 |   208.46 |    0.37 |   141.85 |   234.71 |    0.60 |
| float32  |     128 |      16 |     0.03 |     0.01 |    4.24 |     0.12 |     0.24 |    0.52 |     0.33 |     0.33 |    1.00 |
| float32  |     256 |      16 |     0.04 |     0.01 |    4.68 |     0.14 |     0.21 |    0.69 |     0.34 |     0.32 |    1.07 |
| float32  |     512 |      16 |     0.04 |     0.01 |    3.04 |     0.15 |     0.22 |    0.67 |     0.35 |     0.33 |    1.06 |
| float32  |    1024 |      16 |     0.06 |     0.02 |    2.72 |     0.22 |     0.24 |    0.89 |     0.37 |     0.40 |    0.90 |
| float32  |    2048 |      16 |     0.13 |     0.04 |    3.55 |     0.17 |     0.23 |    0.76 |     0.38 |     0.35 |    1.10 |
| float32  |    4096 |      16 |     0.49 |     0.10 |    5.04 |     0.62 |     0.43 |    1.45 |     1.09 |     0.53 |    2.07 |
| float32  |    8192 |      16 |     1.99 |     0.26 |    7.73 |     2.36 |     1.59 |    1.48 |     4.34 |     1.85 |    2.34 |
| float32  |   16384 |      16 |     6.94 |     0.80 |    8.66 |     9.08 |     5.72 |    1.59 |    15.99 |     6.78 |    2.36 |
| float32  |   32768 |      16 |    30.00 |     2.76 |   10.88 |    36.27 |    22.66 |    1.60 |    66.22 |    25.71 |    2.58 |
| float32  |   65536 |      16 |          N/A |    10.83 |         N/A |          N/A |    91.50 |         N/A |          N/A |   101.89 |         N/A |
| float32  |     128 |      32 |     0.03 |     0.01 |    3.94 |     0.14 |     0.21 |    0.64 |     0.36 |     0.32 |    1.15 |
| float32  |     256 |      32 |     0.04 |     0.01 |    3.27 |     0.15 |     0.20 |    0.75 |     0.35 |     0.31 |    1.10 |
| float32  |     512 |      32 |     0.04 |     0.02 |    2.50 |     0.15 |     0.20 |    0.73 |     0.36 |     0.41 |    0.87 |
| float32  |    1024 |      32 |     0.06 |     0.03 |    2.33 |     0.14 |     0.21 |    0.67 |     0.38 |     0.34 |    1.13 |
| float32  |    2048 |      32 |     0.13 |     0.05 |    2.88 |     0.18 |     0.22 |    0.82 |     0.38 |     0.31 |    1.21 |
| float32  |    4096 |      32 |     0.51 |     0.13 |    3.98 |     0.65 |     0.47 |    1.38 |     1.14 |     0.60 |    1.90 |
| float32  |    8192 |      32 |     1.95 |     0.35 |    5.65 |     2.29 |     1.62 |    1.42 |     4.22 |     2.04 |    2.07 |
| float32  |   16384 |      32 |     7.21 |     1.07 |    6.75 |     9.36 |     6.37 |    1.47 |    16.55 |     7.75 |    2.14 |
| float32  |   32768 |      32 |    31.04 |     3.93 |    7.89 |    37.29 |    25.35 |    1.47 |    68.42 |    29.06 |    2.35 |
| float32  |   65536 |      32 |          N/A |    15.55 |         N/A |          N/A |   104.01 |         N/A |          N/A |   119.16 |         N/A |
| float32  |     128 |      64 |     0.03 |     0.01 |    3.42 |     0.13 |     0.22 |    0.60 |     0.39 |     0.31 |    1.23 |
| float32  |     256 |      64 |     0.04 |     0.01 |    2.60 |     0.15 |     0.29 |    0.50 |     0.43 |     0.37 |    1.15 |
| float32  |     512 |      64 |     0.04 |     0.02 |    1.93 |     0.15 |     0.22 |    0.67 |     0.37 |     0.42 |    0.87 |
| float32  |    1024 |      64 |     0.06 |     0.04 |    1.66 |     0.17 |     0.22 |    0.77 |     0.36 |     0.33 |    1.08 |
| float32  |    2048 |      64 |     0.15 |     0.07 |    2.13 |     0.20 |     0.22 |    0.93 |     0.38 |     0.35 |    1.09 |
| float32  |    4096 |      64 |     0.55 |     0.18 |    3.06 |     0.72 |     0.58 |    1.23 |     1.26 |     0.77 |    1.65 |
| float32  |    8192 |      64 |     2.13 |     0.47 |    4.53 |     2.65 |     2.12 |    1.25 |     4.77 |     2.67 |    1.78 |
| float32  |   16384 |      64 |     8.77 |     1.47 |    5.98 |    11.50 |     9.22 |    1.25 |    20.25 |    11.08 |    1.83 |
| float32  |   32768 |      64 |    39.49 |     5.26 |    7.51 |    48.75 |    39.27 |    1.24 |    88.23 |    43.92 |    2.01 |
| float32  |   65536 |      64 |          N/A |    21.28 |         N/A |          N/A |   152.24 |         N/A |          N/A |   166.54 |         N/A |
| float32  |     128 |     128 |     0.04 |     0.01 |    3.20 |     0.15 |     0.19 |    0.77 |     0.35 |     0.30 |    1.14 |
| float32  |     256 |     128 |     0.05 |     0.02 |    2.75 |     0.16 |     0.21 |    0.75 |     0.42 |     0.38 |    1.11 |
| float32  |     512 |     128 |     0.05 |     0.03 |    1.75 |     0.15 |     0.25 |    0.59 |     0.36 |     0.31 |    1.17 |
| float32  |    1024 |     128 |     0.07 |     0.05 |    1.55 |     0.15 |     0.22 |    0.66 |     0.35 |     0.34 |    1.03 |
| float32  |    2048 |     128 |     0.19 |     0.09 |    2.12 |     0.27 |     0.27 |    1.00 |     0.46 |     0.36 |    1.27 |
| float32  |    4096 |     128 |     0.68 |     0.25 |    2.68 |     0.96 |     0.91 |    1.05 |     1.64 |     1.17 |    1.40 |
| float32  |    8192 |     128 |     2.73 |     0.99 |    2.75 |     3.72 |     3.45 |    1.08 |     6.39 |     4.48 |    1.43 |
| float32  |   16384 |     128 |    10.02 |     2.94 |    3.41 |    14.52 |    13.26 |    1.10 |    24.44 |    16.42 |    1.49 |
| float32  |   32768 |     128 |    44.89 |     9.78 |    4.59 |    59.95 |    57.54 |    1.04 |   104.58 |    65.56 |    1.60 |
| float32  |   65536 |     128 |          N/A |    36.07 |         N/A |          N/A |   213.04 |         N/A |          N/A |   248.24 |         N/A |

结论：

1. Forward 提升明显：两种 dtype 下 forward speedup 都显著大于 1。
2. End-to-end 在 float32 下收益更稳定（多数配置 >1）；在 bfloat16 下波动较大。
3. 最大序列长度（65536）时，float32 常规 attention 全部 OOM，而 FA 可运行，体现出内存优势。
4. 可能由于 backward 不是由 triton 实现的所以有一定性能差距。

## Distributed Data Parallel Training

### `distributed_communication_single_node`

> 实验环境：`RTX 5090 * 6`

| backend | device | procs | size (MB) | mean (ms) | median (ms) | std (ms) | bw (GB/s) |
| ------: | :----: | ----: | --------: | --------: | ----------: | -------: | --------: |
|    gloo |  cpu   |     2 |         1 |     0.887 |       0.888 |    0.067 |     1.127 |
|    gloo |  cpu   |     2 |        10 |     8.207 |       7.919 |    1.178 |     1.218 |
|    gloo |  cpu   |     2 |       100 |    87.496 |      87.207 |    3.269 |     1.143 |
|    gloo |  cpu   |     2 |      1000 |   877.904 |     843.919 |  218.423 |     1.139 |
|    gloo |  cpu   |     4 |         1 |     1.895 |       1.908 |    0.069 |     0.528 |
|    gloo |  cpu   |     4 |        10 |    11.067 |      11.101 |    0.283 |     0.904 |
|    gloo |  cpu   |     4 |       100 |   123.014 |     118.230 |   18.392 |     0.813 |
|    gloo |  cpu   |     4 |      1000 |  1194.454 |    1174.661 |  103.357 |     0.837 |
|    gloo |  cpu   |     6 |         1 |     2.341 |       2.331 |    0.072 |     0.427 |
|    gloo |  cpu   |     6 |        10 |    13.214 |      12.801 |    1.279 |     0.757 |
|    gloo |  cpu   |     6 |       100 |   149.433 |     139.430 |   25.504 |     0.669 |
|    gloo |  cpu   |     6 |      1000 |  1404.927 |    1431.559 |   99.739 |     0.712 |
|    nccl |  cuda  |     2 |         1 |     0.082 |       0.081 |    0.006 |    12.146 |
|    nccl |  cuda  |     2 |        10 |     0.413 |       0.405 |    0.063 |    24.198 |
|    nccl |  cuda  |     2 |       100 |     3.447 |       3.446 |    0.039 |    29.015 |
|    nccl |  cuda  |     2 |      1000 |    34.098 |      34.453 |    0.853 |    29.327 |
|    nccl |  cuda  |     4 |         1 |     0.155 |       0.147 |    0.016 |     6.469 |
|    nccl |  cuda  |     4 |        10 |     0.601 |       0.581 |    0.091 |    16.643 |
|    nccl |  cuda  |     4 |       100 |     5.418 |       5.387 |    0.148 |    18.458 |
|    nccl |  cuda  |     4 |      1000 |    55.907 |      55.938 |    0.208 |    17.887 |
|    nccl |  cuda  |     6 |         1 |     0.282 |       0.298 |    0.038 |     3.542 |
|    nccl |  cuda  |     6 |        10 |     0.775 |       0.734 |    0.462 |    12.911 |
|    nccl |  cuda  |     6 |       100 |     6.895 |       6.871 |    0.172 |    14.502 |
|    nccl |  cuda  |     6 |      1000 |    75.443 |      75.344 |    0.805 |    13.255 |

可以发现，通信后端和设备类型对性能影响显著，CUDA 的 NCCL 表现显著优于 CPU 上的 gloo。随着进程数增加，单次通信延迟整体上升、有效带宽下降，说明同步与拓扑开销开始主导；而消息规模增大时，带宽逐渐趋于饱和，系统从启动/调度开销受限转为链路带宽受限。后端实现、硬件加速能力与并行规模三者共同决定了通信效率。

### `naive_ddp_benchmarking`

> 训练配置：由于 5090 报 OOM 了（峰值显存 > 32 GB），所以使用了 `2 * NVIDIA H800 PCIe` 来跑。
>
> 其余配置和作业要求保持一致，`context_length=128`，`warmup_steps=5`，`measure_steps=10`，`batch_size=4`，对应的 local batch size 即为 2。
>
> 计时口径采用“每个 step 取所有 rank 的最大值（slowest rank）”来代表端到端吞吐时间。

测得平均每步训练时间为 **695.80 ms**（std **0.95 ms**），其中平均梯度通信时间为 **341.02 ms**（std **0.99 ms**），通信时间占总步时的 **49.01%**。这说明在 naive DDP（逐参数 all-reduce）下，通信开销接近总训练开销的一半，是主要瓶颈之一。

### `minimal_ddp_flat_benchmarking`

在与 `naive_ddp_benchmarking` 相同配置下（`1 node x 2 GPUs`，XL，`context_length=128`，`batch_size=4`，`warmup=5`，`measure=10`），对 flatten 后单次 all-reduce 的实现进行 benchmark，结果如下：

| DDP Impl | Step Mean (ms) | Step Std (ms) | Comm Mean (ms) | Comm Std (ms) | Comm Percent |
| :------- | -------------: | ------------: | -------------: | ------------: | -----------: |
| naive    |         695.80 |          0.95 |         341.02 |          0.99 |       49.01% |
| flat     |         711.26 |          2.59 |         356.00 |          3.31 |       50.05% |

本次实验中，`flat` 相比 `naive` 并未带来收益，反而 step 时间增加约 **2.22%**，通信时间增加约 **4.39%**。一个可能原因是 flatten/unflatten 的额外打包与拷贝开销抵消了减少通信调用次数带来的收益，在当前 2 卡单机设置下这一现象更明显。

### `ddp_overlap_individual_parameters_benchmarking`

#### (a) benchmarking

使用 overlap 的一个 step 平均 0.5982s，communication 仅仅占 0.46%（相对 naive 约 1.18x 加速，+17.7%）。在相同配置下，overlap 明显优于 naive/flat，其将“后向结束后剩余通信尾部”从约 49% 降到 0.46%，说明通信与反向计算发生了有效重叠。

#### (b) 

Naive 和 overlap 的 nsys 分析截图如下：

![](https://yangty-pic.oss-cn-beijing.aliyuncs.com/cs336/lab2/naive.png)

![](https://yangty-pic.oss-cn-beijing.aliyuncs.com/cs336/lab2/overlap.png)

可以很明显看到在后者中通信和计算发生了重叠，进而节省了时间。

### `ddp_bucketed_benchmarking`

#### (a) benchmarking

| 实现     | Bucket Size (MB) | Step Time (s/iter) |
| -------- | ---------------: | -----------------: |
| naive    |                - |             0.6966 |
| flat     |                - |             0.7221 |
| overlap  |                - |             0.6021 |
| bucketed |                1 |             0.5945 |
| bucketed |               10 |             0.5976 |
| bucketed |              100 |             0.5923 |
| bucketed |             1000 |             0.6111 |

结论：

- Bucketed-overlap 在 1/10/100MB 下都优于 naive/flat，并且略优于不分桶的 overlap；1000MB 则略退化。
- 本次最优是 100MB（0.5923 s/iter），相对 naive 约 1.176x 加速，相对 overlap 约 1.017x。
- 趋势符合预期：bucket 过大（如 1000MB）会减少可重叠机会，导致尾部通信更明显；bucket 过小则会增加通信调用管理开销。

#### (b) equation

记总参数字节数 $s$，all-reduce 带宽 $w$ bytes/s，每次通信固定开销 $o$ s，bucket 数 $n_b$，单 bucket 大小 $b = s / n_b$。

一次 all-reduce 的时间建模为“固定调用开销 + 带宽项”：
$$
t_{\text{comm}}(b)=o+\frac{b}{w}=o+\frac{s}{n_b w}
$$

在“每个 bucket 的梯度计算时间 = 该 bucket 通信时间”且尽量重叠的假设下，backward 结束后的额外等待可用常见近似模型表示为
$$
T_{\text{over}}(n_b)=n_b\,o+\frac{s}{n_b\,w}
$$

对 $n_b$ 求最小值得到
$$
n_b^*=\sqrt{\frac{s}{o\,w}},\qquad
b^*=\frac{s}{n_b^*}=\sqrt{s\,o\,w}
$$
### `communication_accounting`

#### (a) memory accounting

每个 block 有 $2\times d_{\text{ff}}\times d_{\text{model}}$。总共有 $126$ 个 block 所以总参数量是
$$
P = 126\times2\times16384\times53248=2.19848638464\times10^{11} (\approx219.85\text{B})
$$
FP32 的话，权重 + 梯度 + Adam 状态一共有 $16P \text{ bytes}$ 的内存占用，代入数据计算

- weights：$4P \approx 879.39$ GB

- grads：$4P \approx 879.39$ GB

- Adam state：$8P \approx 1758.79$ GB

- 合计：$\approx 3517.58$ GB（按十进制 GB） 

反传的时候，每个 block 保存的 activation 数量为 $B(d_{\text{model}}+d_{\text{ff}})$，所以 BF16 下
$$
M_{\text{act}}\approx 0.01755\cdot B\text{ GB}
$$
只考虑 FP32 的这些参数的话已经需要 $3517.58/80\approx 44$ 张 H100 了。

#### (b)  FSDP

每张卡：
$$
M_{\text{per}}(N_{\text{FSDP}},B)=\frac{M_{\text{FP32}}}{N_{\text{FSDP}}}
+\frac{1}{2}M_{\text{act}}(B)
+\frac{1}{2}\frac{M_{\text{act}}(B)}{N_{\text{FSDP}}}\\
$$
忽略激活的部分的话，已经需要 $N_{\text{FSDP}}\ge 38$。

#### (c) forward

给定 TPU v5p：$W_{\text{ici}}=2\cdot 9\cdot 10^{10}$，$C=4.6\cdot 10^{14}$，$M_X=2,M_Y=1,X=16,Y=4$。取常见做法：TP 沿 $Y$ 切 $d_{\text{ff}}$，并假设通信均为 BF16。

每设备每 block 计算量：
$$
F_{\text{block}}=\frac{4B,d_{\text{model}}d_{\text{ff}}}{Y},\qquad t_{\text{comp}}=\frac{F_{\text{block}}}{C}
$$
每设备每 block 通信量：

- FSDP 权重 all-gather：两层（$M_X=2$），每层需 all-gather 大小 $\frac{d_{\text{model}}d_{\text{ff}}}{Y}$，ring 近似因子 $\frac{X-1}{X}$：
  $$
  \text{comm}*X=M_X\frac{X-1}{X}\frac{d*{\text{model}}d_{\text{ff}}}{Y}\cdot 2
  $$
  
- TP 激活 all-reduce：一次（$M_Y=1$），消息 $Bd_{\text{model}}$，ring 因子 $2\frac{Y-1}{Y}$：
  $$
  \text{comm}*Y=M_Y\cdot 2\frac{Y-1}{Y}\cdot (Bd*{\text{model}})\cdot 2
  $$

于是
$$
t_{\text{comm}}=\frac{\text{comm}_X+\text{comm}*Y}{W*{\text{ici}}}
$$
**compute-bound 条件** $t_{\text{comp}}\ge t_{\text{comm}}$ 解得临界 batch：
$$
B^\star=\frac{\text{comm}*X/W*{\text{ici}}}{\frac{4d_{\text{model}}d_{\text{ff}}}{YC}-\frac{M_Y\cdot 2\frac{Y-1}{Y}\cdot d_{\text{model}}\cdot 2}{W_{\text{ici}}}}
\approx 2798.81 \approx 2.8\times 10^3
$$
**overall batch**：若数据并行复制数取 $M_XM_Y=2$，则 $B_{\text{global}}\approx B^\star\cdot 2\approx 5600$。

#### (d) tricks

* **通信-计算重叠（overlap）**：对下一层权重 all-gather 进行 prefetch/异步通信，把 $\text{comm}_X$ 隐藏在 matmul 计算下。
* **降低通信字节数**：权重/激活通信用更低精度（如 FP8/压缩），从而近似线性降低 $B^\star$。
* **减少权重反复 all-gather**：缓存若干层已聚合权重、或对部分层暂时不分片（在可接受显存内）以降低 $\text{comm}_X$ 的常数项。
* **改变切分策略以交换通信对象**：在某些形状下，用“少 gather 权重、多通信激活”的 TP/并行布局可能更优（本质是让大的那一项更小）。
* **提升有效带宽/collective 实现**：更好的拓扑映射、集合通信优化可提升等效 $W_{\text{ici}}$，直接降低 $B^\star$。

## Optimizer State Sharding

### `optimizer_state_sharding`

#### (a) peak memory

在标准配置（1 node, 2 GPUs, XL）下：

  - after model init：两者基本一致，约 7.62 GiB
  - before optimizer step：两者基本一致，约 15.31 GiB
  - after optimizer step：
      - non-sharded：38.18 GiB
      - sharded：26.93 GiB
      - 可以发现 optimizer state sharding 在 optimizer.step() 后的峰值显存减少约 11.25 GiB（约 29.47%）。

参数和梯度在两种设置下都基本不变（每 rank 参数约 7.44 GiB，梯度约 7.44 GiB）；主要差异来自 optimizer state：

  - non-sharded 每 rank optimizer state 约 14.89 GiB
  - sharded 每 rank optimizer state 约 7.32-7.56 GiB（约减半）
    这与“只在本 rank 保存部分参数的 optimizer state”预期一致。

#### (b) training speed

每步耗时（step mean）：

- non-sharded：0.6334 s/iter
- sharded：0.8161 s/iter
- sharded 相比 non-sharded 变慢约 28.85%。

原因是 sharded 版本在每次本地更新后需要跨 rank 广播已更新参数来重新同步模型，这引入了额外通信与同步开销。

#### (c) ours vs. ZeRO-1

我们的实现是：每张卡仍然保留**完整参数 + 完整梯度**，只把 **optimizer state** 按参数切片到各 rank；每步只更新自己那一片参数，然后再把更新后的参数片**广播**给其它 rank 来保持参数一致——这会在常规 DDP 的梯度 all-reduce 之外，再额外产生一次“同步参数”的通信（规模约为一次参数量 $\Psi$）。

ZeRO Stage 1在“只切 optimizer state、参数/梯度仍复制”的点上和我们做的目标一致，但它用论文里的**动态通信调度**把“需要的通信”重排：把梯度平均视作 reduce-scatter+all-gather（总通信量 $2\Psi$），并用 **reduce-scatter (梯度)** + **all-gather (更新后参数分片)** 来做到**总体通信量仍 $\approx 2\Psi$（不比标准 DP 更高）**，而不是在 all-reduce 之后再额外 broadcast 一遍参数。
